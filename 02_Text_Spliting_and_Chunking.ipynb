{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93a439d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Callable, Optional\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c703e605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# document class from previous module\n",
    "class Document:\n",
    "    def __init__(self, pageContent: str, metadata: dict, id: str):\n",
    "        self.pageContent = pageContent  # raw text from pdf\n",
    "        self.metadata: dict = metadata  # pdf metadata (author, date ...)\n",
    "        self.id = id  # pdf id\n",
    "\n",
    "    def __repr__(self):\n",
    "        meta_repr = {k: v for k, v in self.metadata.items() if k not in ('full_text',)}\n",
    "        return f\"Document(id={self.id!r}, chunk={self.metadata.get('chunk')}, total={self.metadata.get('totalChunks')}, len={len(self.pageContent)})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142ea9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextSplitter:\n",
    "    def __init__(self, chunkSize: int, chunkOverlap: int, lengthfunction: Optional[Callable[[str], int]] = None, keepSeperator:bool = False):\n",
    "        self.chunkSize = chunkSize\n",
    "        self.chunkOverlap = chunkOverlap\n",
    "        self.lengthfunction = lengthfunction or (lambda t: len(t))\n",
    "        self.keepSeperator = keepSeperator\n",
    "\n",
    "        if chunkOverlap > chunkSize:\n",
    "            raise ValueError(f\"chunkOverlap cannot be greater than chunkSize: {chunkSize} < {chunkOverlap}\")\n",
    "        \n",
    "    def splitText(self, text: str):\n",
    "        # forbidden, can only be implemented by the sub classes\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def splitDocuments(self, documents: List[Document]) -> List[Document]:\n",
    "        \"\"\"Split a list of Documents into chunks of Documents\"\"\"\n",
    "        chunks: List[Document] = []\n",
    "        for doc in documents:\n",
    "            split_doc = self.createDocuments([doc.pageContent], [doc.metadata], source_id = doc.id)\n",
    "            chunks.append(split_doc)\n",
    "        return chunks # chunks of documents\n",
    "\n",
    "    def createDocuments(self, texts: List[str], metadatas: Optional[List[Dict]] = [], source_id: Optional[str] = None) -> List[Document]:\n",
    "        metadatas = metadatas or [{}]\n",
    "        doucments: List[Document] = []\n",
    "        for i, text in enumerate(texts):\n",
    "            metadata = metadatas[i] if i < len(metadatas) else {}\n",
    "            chunks = self.splitText(text)\n",
    "            total = len(chunks)\n",
    "\n",
    "            for j, chunk in enumerate(chunks):\n",
    "                chunk_id = f\"{source_id or metadata.get('id', str(uuid.uuid4()))}_chunk_{j}\"\n",
    "                doc_meta = {**metadata, \"chunk\": j, \"totalChunk\": total}\n",
    "                doucments.append(Document(chunk, doc_meta, chunk_id))\n",
    "\n",
    "        return doucments\n",
    "    \n",
    "    def mergeSplits(self, splits: List[str], separator: str) -> List[str]:\n",
    "        \"\"\"Merge splits of text into chunks of size <= chunkSize\"\"\"\n",
    "        chunks: List[str] = []\n",
    "        current: List[str] = []\n",
    "        length = 0\n",
    "\n",
    "        for split in splits:\n",
    "            split_length = self.lengthfunction(split)\n",
    "            extra_length = len(separator) if current else 0\n",
    "            if length + split_length + extra_length >= self.chunkSize:\n",
    "                if current:\n",
    "                    joined = self.joinSplits(current, separator)\n",
    "                    if joined:\n",
    "                        chunks.append(joined)\n",
    "                while length > self.chunkOverlap and current:\n",
    "                    removed = current.pop(0)\n",
    "                    length -= (self.lengthfunction(removed) + (len(separator) if current else 0))\n",
    "            current.append(split)\n",
    "            length += split_length + (len(separator) if len(current) > 1 else 0)\n",
    "        if current:\n",
    "            joined = self.joinSplits(current, separator)\n",
    "            if joined:\n",
    "                chunks.append(joined)\n",
    "        return [c for c in chunks if c]\n",
    "        \n",
    "\n",
    "    def joinSplits(self, splits: List[str], separator: str) -> Optional[str]:\n",
    "        \"\"\"Join the temp splits\"\"\"\n",
    "        text = separator.join(splits).strip()\n",
    "        return text if text else None\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d44a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecursiveCharacterTextSplitter(TextSplitter):\n",
    "    def __init__(self, separators: Optional[List[str]] = None,\n",
    "                 chunkSize: int = 1000,\n",
    "                 chunkOverlap: int = 200,\n",
    "                 lengthfunction: Optional[Callable[[str], int]] = None,\n",
    "                 keepSeparator: bool = False):\n",
    "        if separators is None:\n",
    "            separators = [\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "        super().__init__(chunkSize=chunkSize, chunkOverlap=chunkOverlap, lengthfunction=lengthfunction, keepSeperator=keepSeparator)\n",
    "        self.separators =separators\n",
    "\n",
    "    def splitText(self, text: str) -> List[str]:\n",
    "        \"\"\"Recursively split texts based on the next separator from the list of separators\"\"\"\n",
    "        final_chunk: List[str] = []\n",
    "        separator = self.separators[-1]\n",
    "        next_separator: List[str] = []\n",
    "\n",
    "        for i, sep in enumerate(self.separators):\n",
    "            if sep and sep in text:\n",
    "                separator = sep\n",
    "                next_separator = self.separators[i+1:]\n",
    "                break\n",
    "\n",
    "            if sep == \"\" and sep == \"\":\n",
    "                separator = \"\"\n",
    "                next_separator = []\n",
    "\n",
    "        splits = [s for s in text.split(separator) if s is not None and (s.strip() if separator else s != \"\")]\n",
    "        temp: List[str] = []\n",
    "\n",
    "        for s in splits:\n",
    "            if self.lengthfunction(s) <= self.chunkSize:\n",
    "                temp.append(s)\n",
    "            else:\n",
    "                if temp:\n",
    "                    final_chunk.extend(self.mergeSplits(temp, separator))\n",
    "                    temp = []\n",
    "                if not separator:\n",
    "                    final_chunk.append(s)\n",
    "                else:\n",
    "                    recursive = RecursiveCharacterTextSplitter(separators=next_separator,\n",
    "                                                               chunkSize=self.chunkSize,\n",
    "                                                               chunkOverlap=self.chunkOverlap,\n",
    "                                                               lengthfunction=self.lengthfunction,\n",
    "                                                               keepSeparator=self.keepSeperator)\n",
    "                    final_chunk.extend(recursive.splitText(s))\n",
    "                \n",
    "        if temp:\n",
    "            final_chunk.extend(self.mergeSplits(temp, separator))\n",
    "        return final_chunk\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c36407d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULT: created 5 documents (chunks)\n",
      "\n",
      "Document(id='pdf1_chunk_0', chunk=0, total=None, len=27)\n",
      "-> content preview: 'This is the first paragraph'\n",
      "-> metadata: {'source': 'sample.pdf', 'author': 'tester', 'chunk': 0, 'totalChunk': 5}\n",
      "---\n",
      "Document(id='pdf1_chunk_1', chunk=1, total=None, len=24)\n",
      "-> content preview: 'It has several sentences'\n",
      "-> metadata: {'source': 'sample.pdf', 'author': 'tester', 'chunk': 1, 'totalChunk': 5}\n",
      "---\n",
      "Document(id='pdf1_chunk_2', chunk=2, total=None, len=45)\n",
      "-> content preview: 'Each sentence helps demonstrate the splitter.'\n",
      "-> metadata: {'source': 'sample.pdf', 'author': 'tester', 'chunk': 2, 'totalChunk': 5}\n",
      "---\n",
      "Document(id='pdf1_chunk_3', chunk=3, total=None, len=21)\n",
      "-> content preview: 'Second paragraph here'\n",
      "-> metadata: {'source': 'sample.pdf', 'author': 'tester', 'chunk': 3, 'totalChunk': 5}\n",
      "---\n",
      "Document(id='pdf1_chunk_4', chunk=4, total=None, len=32)\n",
      "-> content preview: 'It is shorter, but still useful.'\n",
      "-> metadata: {'source': 'sample.pdf', 'author': 'tester', 'chunk': 4, 'totalChunk': 5}\n",
      "---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'print(\"\\n-- CharacterTextSplitter demo --\")\\ncsplitter = CharacterTextSplitter(chunk_size=80, chunk_overlap=20, separator=\"\\n\\n\")\\ncchunks = csplitter.split_documents([pdf_doc])\\nprint(\"Character splitter produced\", len(cchunks), \"chunks. First chunk preview:\")\\nprint(cchunks[0].pageContent[:200])\\n\\nprint(\"\\n-- TokenTextSplitter demo --\")\\ntsplitter = TokenTextSplitter(chunk_size=100, chunk_overlap=20)\\ntchunks = tsplitter.split_documents([pdf_doc])\\nprint(\"Token splitter produced\", len(tchunks), \"chunks. First chunk approx tokens:\", tsplitter.length_function(tchunks[0].pageContent))'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text = (\n",
    "    \"This is the first paragraph. It has several sentences. Each sentence helps demonstrate the splitter.\\n\\n\"\n",
    "    \"Second paragraph here. It is shorter, but still useful.\"\n",
    ")\n",
    "pdf_doc = Document(pageContent=sample_text, metadata={\"source\": \"sample.pdf\", \"author\": \"tester\"}, id=\"pdf1\")\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunkSize=50, chunkOverlap=10)\n",
    "chunked_docs = splitter.splitDocuments([pdf_doc])\n",
    "\n",
    "print(\"RESULT: created\", sum(len(docs) for docs in chunked_docs), \"documents (chunks)\\n\")\n",
    "for docs in chunked_docs:\n",
    "    for d in docs:\n",
    "        print(d)\n",
    "        print(\"-> content preview:\", repr(d.pageContent))\n",
    "        print(\"-> metadata:\", d.metadata)\n",
    "        print(\"---\")\n",
    "\n",
    "\"\"\"print(\"\\n-- CharacterTextSplitter demo --\")\n",
    "csplitter = CharacterTextSplitter(chunk_size=80, chunk_overlap=20, separator=\"\\n\\n\")\n",
    "cchunks = csplitter.split_documents([pdf_doc])\n",
    "print(\"Character splitter produced\", len(cchunks), \"chunks. First chunk preview:\")\n",
    "print(cchunks[0].pageContent[:200])\n",
    "\n",
    "print(\"\\n-- TokenTextSplitter demo --\")\n",
    "tsplitter = TokenTextSplitter(chunk_size=100, chunk_overlap=20)\n",
    "tchunks = tsplitter.split_documents([pdf_doc])\n",
    "print(\"Token splitter produced\", len(tchunks), \"chunks. First chunk approx tokens:\", tsplitter.length_function(tchunks[0].pageContent))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0d925e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
